{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd612db-bdfd-4f48-bb03-472f72849fc2",
   "metadata": {},
   "source": [
    "The equality you're referring to in the context of XGBoost is based on how the objective function is updated in each iteration. To understand why this equality holds, let's break it down step by step.\r\n",
    "\r\n",
    "### Objective Function in XGBoost\r\n",
    "\r\n",
    "1. **General Form**:\r\n",
    "   The objective function \\( Q(t) \\) at iteration \\( t \\) is composed of two parts:\r\n",
    "   - The loss function \\( l \\), which measures the difference between the predicted values and the actual target values.\r\n",
    "   - The regularization term \\( \\Omega \\), which penalizes the complexity of themode$$    \\[\r\n",
    "   Q(t) = \\sum_{i=1}^{n} l(y_i, \\a{y}_i^{(t)}) + \\sum {j=1}^{t} \\$$\n",
    "_j)\r\n",
    "   \\]\r\n",
    "\r\n",
    "2. **Expansion at Iteration \\( t \\)**:\r\n",
    "   When adding a new tree \\( f_t \\) to the model, the predicted value \\( \\hat{y}_i^{(t)} \\) is updated by adding the prediction of the new tree to the previous prediction \\( \\hat{y}_i^{(t-1)} \\):\r\n",
    "\r\n",
    "   \\[\r\n",
    "   \\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i)\r\n",
    "   \\]\r\n",
    "\r\n",
    "3. **Objective Function Update**:\r\n",
    "   Therefore, the objective function at iteration \\( t \\) can be expressed as:\r\n",
    "\r\n",
    "   \\[\r\n",
    "   Q(t) = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t)}) + \\sum_{j=1}^{t} \\Omega(f_j)\r\n",
    "   \\]\r\n",
    "\r\n",
    "   Since \\(\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i)\\), we can substitute this into the loss function:\r\n",
    "\r\n",
    "   \\[\r\n",
    "   Q(t) = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) + \\Omega(f_t) + \\sum_{j=1}^{t-1} \\Omega(f_j)\r\n",
    "   \\]\r\n",
    "\r\n",
    "   Notice that the regularization term \\(\\Omega(f_t)\\) for the new tree \\( f_t \\) is separated out, while the regularization terms for the previous trees \\(\\sum_{j=1}^{t-1} \\Omega(f_j)\\) are grouped together.\r\n",
    "\r\n",
    "4. **Constant Term**:\r\n",
    "   The sum of the regularization terms for all previous trees \\( \\sum_{j=1}^{t-1} \\Omega(f_j) \\) is a constant with respect to the current iteration \\( t \\). Therefore, it can be considered a constant in the context of the optimization problem at iteration \\( t \\):\r\n",
    "\r\n",
    "   \\[\r\n",
    "   Q(t) = \\sum_{i=1}^{n} l(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) + \\Omega(f_t) + \\text{constant}\r\n",
    "   \\]\r\n",
    "\r\n",
    "### Why the Equality Holds\r\n",
    "\r\n",
    "The equality holds because of the way the objective function is decomposed and updated at each iteration. Specifically:\r\n",
    "\r\n",
    "1. **Incremental Update**:\r\n",
    "   - The prediction \\(\\hat{y}_i^{(t)}\\) is updated incrementally by adding the prediction of the new tree \\( f_t(x_i) \\) to the previous prediction \\(\\hat{y}_i^{(t-1)}\\).\r\n",
    "   - This incremental update is reflected directly in the loss function.\r\n",
    "\r\n",
    "2. **Regularization Term**:\r\n",
    "   - The regularization term for the new tree \\(\\Omega(f_t)\\) is added explicitly, while the regularization terms for the previous trees \\(\\sum_{j=1}^{t-1} \\Omega(f_j)\\) remain constant with respect to the current iteration.\r\n",
    "   - The constant term \\(\\sum_{j=1}^{t-1} \\Omega(f_j)\\) does not affect the optimization process for the current iteration, as it does not depend on the new tree \\( f_t \\).\r\n",
    "\r\n",
    "3. **Objective Function Simplification**:\r\n",
    "   - By isolating the components that change at each iteration (i.e., the loss function with the new prediction and the regularization term for the new tree), we can focus on optimizing the relevant parts without needing to recompute the entire objective function from scratch.\r\n",
    "\r\n",
    "This approach allows for efficient optimization and clear understanding of how each new tree contributes to the overall model, both in terms of reducing the prediction error and controlling model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a19bb71-99a8-4210-b394-d016e8ba42e4",
   "metadata": {},
   "source": [
    "Certainly! Let's clarify the difference between \\( w_j \\) and \\( w_{q(x)} \\) in the context of XGBoost and its objective function.\n",
    "\n",
    "### Definitions\n",
    "\n",
    "1. **\\( w_j \\)**:\n",
    "   - \\( w_j \\) is the weight (or score) assigned to the \\( j \\)-th leaf in the decision tree.\n",
    "   - Each leaf in a tree has a unique weight \\( w_j \\), which represents the value predicted for all data points that fall into that leaf.\n",
    "\n",
    "2. **\\( w_{q(x)} \\)**:\n",
    "   - \\( w_{q(x)} \\) refers to the weight of the leaf to which the data point \\( x \\) is assigned.\n",
    "   - The function \\( q(x) \\) maps a data point \\( x \\) to the index of the leaf it falls into. Thus, \\( w_{q(x)} \\) is the weight of that specific leaf.\n",
    "\n",
    "### Explanation\n",
    "\n",
    "When constructing a decision tree, each data point \\( x_i \\) is assigned to a leaf. The tree determines which leaf a data point belongs to based on its features. \n",
    "\n",
    "- \\( q(x_i) \\): This function determines the leaf index \\( j \\) for data point \\( x_i \\).\n",
    "- \\( w_{q(x_i)} \\): This is the weight of the leaf where \\( x_i \\) ends up.\n",
    "\n",
    "In simpler terms:\n",
    "- \\( w_j \\) is a general term for the weight of the \\( j \\)-th leaf.\n",
    "- \\( w_{q(x_i)} \\) is the specific weight of the leaf to which the \\( i \\)-th data point \\( x_i \\) is assigned.\n",
    "\n",
    "### Example\n",
    "\n",
    "Imagine we have a tree with three leaves:\n",
    "\n",
    "- Leaf 1 has weight \\( w_1 \\).\n",
    "- Leaf 2 has weight \\( w_2 \\).\n",
    "- Leaf 3 has weight \\( w_3 \\).\n",
    "\n",
    "If the function \\( q(x_i) \\) assigns data point \\( x_1 \\) to leaf 2, then \\( w_{q(x_1)} \\) is \\( w_2 \\). Similarly, if \\( q(x_2) \\) assigns data point \\( x_2 \\) to leaf 1, then \\( w_{q(x_2)} \\) is \\( w_1 \\).\n",
    "\n",
    "### In the Objective Function\n",
    "\n",
    "The objective function in XGBoost with the Taylor expansion looks like this:\n",
    "\n",
    "\\[\n",
    "Q(t) = \\sum_{i=1}^{n} \\left[ l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i) \\right] + \\Omega(f_t) + \\text{constant}\n",
    "\\]\n",
    "\n",
    "Where \\( f_t(x_i) = w_{q(x_i)} \\).\n",
    "\n",
    "So, the terms involving the weights in the objective function can be written as:\n",
    "\n",
    "\\[\n",
    "\\sum_{i=1}^{n} \\left[ g_i w_{q(x_i)} + \\frac{1}{2} h_i w_{q(x_i)}^2 \\right]\n",
    "\\]\n",
    "\n",
    "Now, grouping by leaves (since all data points in the same leaf \\( j \\) have the same weight \\( w_j \\)):\n",
    "\n",
    "\\[\n",
    "\\sum_{j=1}^{T} \\sum_{i \\in I_j} \\left[ g_i w_j + \\frac{1}{2} h_i w_j^2 \\right]\n",
    "\\]\n",
    "\n",
    "Let \\( G_j = \\sum_{i \\in I_j} g_i \\) and \\( H_j = \\sum_{i \\in I_j} h_i \\):\n",
    "\n",
    "\\[\n",
    "Q(t) = \\sum_{j=1}^{T} \\left[ G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2 \\right] + \\gamma T + \\text{constant}\n",
    "\\]\n",
    "\n",
    "### Summary\n",
    "\n",
    "- \\( w_j \\): The weight of the \\( j \\)-th leaf.\n",
    "- \\( w_{q(x_i)} \\): The weight of the leaf where data point \\( x_i \\) is assigned, which corresponds to the leaf's weight.\n",
    "\n",
    "In the objective function, \\( w_{q(x_i)} \\) is used to denote the specific leaf weight for the data point \\( x_i \\). When summing over all data points, we group them by their respective leaves and use \\( w_j \\) to simplify the expression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d892ea-e9aa-41b1-ba3e-0ea25e5b2b6b",
   "metadata": {},
   "source": [
    "Sure! Let's go through an example of building a decision tree with a depth of 3, and how we use the weights \\( w_j \\) in XGBoost. We'll also illustrate how \\( w_j \\) is determined during the optimization process.\n",
    "\n",
    "### Example Tree with Depth 3\n",
    "\n",
    "A decision tree with depth 3 has \\( 2^3 = 8 \\) leaves (since each node splits into two, and we have three levels of splits).\n",
    "\n",
    "### Data\n",
    "\n",
    "Let's consider a simple dataset with 8 data points and one feature:\n",
    "\n",
    "| Data Point | Feature \\( x \\) | Target \\( y \\) |\n",
    "|------------|-----------------|----------------|\n",
    "| 1          | 0.1             | 0              |\n",
    "| 2          | 0.4             | 1              |\n",
    "| 3          | 0.5             | 0              |\n",
    "| 4          | 0.7             | 1              |\n",
    "| 5          | 0.8             | 0              |\n",
    "| 6          | 1.0             | 1              |\n",
    "| 7          | 1.2             | 0              |\n",
    "| 8          | 1.4             | 1              |\n",
    "\n",
    "### Building the Tree\n",
    "\n",
    "1. **Level 1 Split**:\n",
    "   - Split the data based on the feature \\( x \\). For simplicity, let's say the first split is at \\( x = 0.5 \\).\n",
    "\n",
    "2. **Level 2 Split**:\n",
    "   - For data points where \\( x \\leq 0.5 \\), split again at \\( x = 0.3 \\).\n",
    "   - For data points where \\( x > 0.5 \\), split again at \\( x = 0.9 \\).\n",
    "\n",
    "3. **Level 3 Split**:\n",
    "   - Continue splitting each subset of data points further.\n",
    "\n",
    "Let's say our splits result in the following leaves:\n",
    "\n",
    "- Leaf 1: \\( x \\leq 0.3 \\) (Data points 1, 2)\n",
    "- Leaf 2: \\( 0.3 < x \\leq 0.5 \\) (Data point 3)\n",
    "- Leaf 3: \\( 0.5 < x \\leq 0.7 \\) (Data point 4)\n",
    "- Leaf 4: \\( 0.7 < x \\leq 0.8 \\) (Data point 5)\n",
    "- Leaf 5: \\( 0.8 < x \\leq 1.0 \\) (Data point 6)\n",
    "- Leaf 6: \\( 1.0 < x \\leq 1.2 \\) (Data point 7)\n",
    "- Leaf 7: \\( 1.2 < x \\leq 1.4 \\) (Data point 8)\n",
    "\n",
    "### Weights \\( w_j \\)\n",
    "\n",
    "The weights \\( w_j \\) are not predetermined; they are the values we want to optimize. These weights represent the predicted value for each leaf. \n",
    "\n",
    "### Optimization Process\n",
    "\n",
    "Let's walk through the process of optimizing the weights using the Taylor expansion and regularization.\n",
    "\n",
    "#### Step 1: Initialize Predictions\n",
    "\n",
    "Start with initial predictions, say \\(\\hat{y}_i^{(0)} = 0\\) for all \\( i \\).\n",
    "\n",
    "#### Step 2: Compute Gradients and Hessians\n",
    "\n",
    "For each data point, compute the gradients \\( g_i \\) and Hessians \\( h_i \\):\n",
    "\n",
    "\\[\n",
    "g_i = \\frac{\\partial l(y_i, \\hat{y}_i^{(t-1)})}{\\partial \\hat{y}_i}\n",
    "\\]\n",
    "\\[\n",
    "h_i = \\frac{\\partial^2 l(y_i, \\hat{y}_i^{(t-1)})}{\\partial \\hat{y}_i^2}\n",
    "\\]\n",
    "\n",
    "For simplicity, let's assume a squared loss function \\( l(y, \\hat{y}) = (y - \\hat{y})^2 \\):\n",
    "\n",
    "\\[\n",
    "g_i = -2 (y_i - \\hat{y}_i)\n",
    "\\]\n",
    "\\[\n",
    "h_i = 2\n",
    "\\]\n",
    "\n",
    "#### Step 3: Sum Gradients and Hessians for Each Leaf\n",
    "\n",
    "Sum the gradients and Hessians for each leaf \\( j \\):\n",
    "\n",
    "\\[\n",
    "G_j = \\sum_{i \\in I_j} g_i\n",
    "\\]\n",
    "\\[\n",
    "H_j = \\sum_{i \\in I_j} h_i\n",
    "\\]\n",
    "\n",
    "Let's calculate these for our example:\n",
    "\n",
    "- Leaf 1: \\( G_1 = g_1 + g_2 \\), \\( H_1 = h_1 + h_2 \\)\n",
    "- Leaf 2: \\( G_2 = g_3 \\), \\( H_2 = h_3 \\)\n",
    "- Leaf 3: \\( G_3 = g_4 \\), \\( H_3 = h_4 \\)\n",
    "- Leaf 4: \\( G_4 = g_5 \\), \\( H_4 = h_5 \\)\n",
    "- Leaf 5: \\( G_5 = g_6 \\), \\( H_5 = h_6 \\)\n",
    "- Leaf 6: \\( G_6 = g_7 \\), \\( H_6 = h_7 \\)\n",
    "- Leaf 7: \\( G_7 = g_8 \\), \\( H_7 = h_8 \\)\n",
    "\n",
    "#### Step 4: Optimize Weights for Each Leaf\n",
    "\n",
    "The optimal weight \\( w_j \\) for each leaf is given by:\n",
    "\n",
    "\\[\n",
    "w_j = -\\frac{G_j}{H_j + \\lambda}\n",
    "\\]\n",
    "\n",
    "#### Step 5: Update Predictions\n",
    "\n",
    "Update the predictions for each data point based on the leaf it falls into:\n",
    "\n",
    "\\[\n",
    "\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i) = \\hat{y}_i^{(t-1)} + w_{q(x_i)}\n",
    "\\]\n",
    "\n",
    "### Summary of Weights \\( w_j \\)\n",
    "\n",
    "- \\( w_j \\) are the weights assigned to each leaf in the tree.\n",
    "- They are not set beforehand but are optimized during the training process.\n",
    "- The optimization involves computing the sum of gradients and Hessians for data points in each leaf and then calculating the optimal weight for each leaf.\n",
    "\n",
    "### Example Calculations\n",
    "\n",
    "Assume \\(\\lambda = 1\\):\n",
    "\n",
    "1. Compute gradients and Hessians for each data point.\n",
    "2. Sum them for each leaf.\n",
    "3. Calculate \\( w_j \\) for each leaf using the formula.\n",
    "4. Update predictions for all data points based on the optimized leaf weights.\n",
    "\n",
    "Let's do a partial calculation for illustration (assuming some arbitrary gradient values for simplicity):\n",
    "\n",
    "- Leaf 1: Data points 1 and 2\n",
    "  - \\( g_1 = -2 \\cdot (0 - 0) = 0 \\)\n",
    "  - \\( g_2 = -2 \\cdot (1 - 0) = -2 \\)\n",
    "  - \\( G_1 = 0 + (-2) = -2 \\)\n",
    "  - \\( H_1 = 2 + 2 = 4 \\)\n",
    "  - \\( w_1 = -\\frac{-2}{4 + 1} = \\frac{2}{5} = 0.4 \\)\n",
    "\n",
    "- Leaf 2: Data point 3\n",
    "  - \\( g_3 = -2 \\cdot (0 - 0) = 0 \\)\n",
    "  - \\( G_2 = 0 \\)\n",
    "  - \\( H_2 = 2 \\)\n",
    "  - \\( w_2 = -\\frac{0}{2 + 1} = 0 \\)\n",
    "\n",
    "Repeat for other leaves similarly.\n",
    "\n",
    "In this example, the weights \\( w_j \\) are optimized based on the gradients and Hessians of the loss function, ensuring that the new tree improves the model by reducing the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0467213c-b502-47d6-b6a1-805a36a9f1c3",
   "metadata": {},
   "source": [
    "Sure, let's go through the example step-by-step with specific calculations to see how \\( w_{q(x_i)} \\) is used in the objective function for a tree with depth 3. We'll focus on computing \\( w_{q(x_i)} \\) for each leaf, given some initial values and loss function.\r\n",
    "\r\n",
    "### Example Dataset\r\n",
    "\r\n",
    "We'll use the same dataset:\r\n",
    "\r\n",
    "| Data Point | Feature \\( x \\) | Target \\( y \\) |\r\n",
    "|------------|-----------------|----------------|\r\n",
    "| 1          | 0.1             | 0              |\r\n",
    "| 2          | 0.4             | 1              |\r\n",
    "| 3          | 0.5             | 0              |\r\n",
    "| 4          | 0.7             | 1              |\r\n",
    "| 5          | 0.8             | 0              |\r\n",
    "| 6          | 1.0             | 1              |\r\n",
    "| 7          | 1.2             | 0              |\r\n",
    "| 8          | 1.4             | 1              |\r\n",
    "\r\n",
    "### Decision Tree Structure (Depth 3)\r\n",
    "\r\n",
    "Assume the following splits:\r\n",
    "\r\n",
    "1. Level 1: \\( x \\leq 0.5 \\)\r\n",
    "   - Left: \\( x \\leq 0.5 \\) (Data points 1, 2, 3)\r\n",
    "   - Right: \\( x > 0.5 \\) (Data points 4, 5, 6, 7, 8)\r\n",
    "\r\n",
    "2. Level 2:\r\n",
    "   - Left child of level 1: \\( x \\leq 0.3 \\)\r\n",
    "     - Left: \\( x \\leq 0.3 \\) (Data point 1)\r\n",
    "     - Right: \\( 0.3 < x \\leq 0.5 \\) (Data points 2, 3)\r\n",
    "   - Right child of level 1: \\( x \\leq 1.0 \\)\r\n",
    "     - Left: \\( 0.5 < x \\leq 0.7 \\) (Data point 4)\r\n",
    "     - Right: \\( 0.7 < x \\leq 1.0 \\) (Data points 5, 6)\r\n",
    "\r\n",
    "3. Level 3:\r\n",
    "   - Further split each node as needed, but for simplicity, we'll assume the splits lead to the final 8 leaves directly.\r\n",
    "\r\n",
    "### Initial Predictions\r\n",
    "\r\n",
    "Assume initial predictions \\( \\hat{y}_i^{(0)} = 0 \\) for all \\( i \\).\r\n",
    "\r\n",
    "### Compute Gradients and Hessians\r\n",
    "\r\n",
    "Using squared loss \\( l(y, \\hat{y}) = (y - \\hat{y})^2 \\):\r\n",
    "\r\n",
    "\\[\r\n",
    "g_i = -2 (y_i - \\hat{y}_i)\r\n",
    "\\]\r\n",
    "\\[\r\n",
    "h_i = 2\r\n",
    "\\]\r\n",
    "\r\n",
    "### Compute \\( g_i \\) and \\( h_i \\) for each data point\r\n",
    "\r\n",
    "\\[\r\n",
    "\\begin{align*}\r\n",
    "g_1 &= -2 (0 - 0) = 0 & h_1 &= 2 \\\\\r\n",
    "g_2 &= -2 (1 - 0) = -2 & h_2 &= 2 \\\\\r\n",
    "g_3 &= -2 (0 - 0) = 0 & h_3 &= 2 \\\\\r\n",
    "g_4 &= -2 (1 - 0) = -2 & h_4 &= 2 \\\\\r\n",
    "g_5 &= -2 (0 - 0) = 0 & h_5 &= 2 \\\\\r\n",
    "g_6 &= -2 (1 - 0) = -2 & h_6 &= 2 \\\\\r\n",
    "g_7 &= -2 (0 - 0) = 0 & h_7 &= 2 \\\\\r\n",
    "g_8 &= -2 (1 - 0) = -2 & h_8 &= 2 \\\\\r\n",
    "\\end{align*}\r\n",
    "\\]\r\n",
    "\r\n",
    "### Sum Gradients and Hessians for Each Leaf\r\n",
    "\r\n",
    "Letâ€™s assume our tree ends up with the following leaves and their assigned data points:\r\n",
    "\r\n",
    "- Leaf 1: \\( x \\leq 0.3 \\) (Data point 1)\r\n",
    "- Leaf 2: \\( 0.3 < x \\leq 0.5 \\) (Data points 2, 3)\r\n",
    "- Leaf 3: \\( 0.5 < x \\leq 0.7 \\) (Data point 4)\r\n",
    "- Leaf 4: \\( 0.7 < x \\leq 0.8 \\) (Data point 5)\r\n",
    "- Leaf 5: \\( 0.8 < x \\leq 1.0 \\) (Data point 6)\r\n",
    "- Leaf 6: \\( 1.0 < x \\leq 1.2 \\) (Data point 7)\r\n",
    "- Leaf 7: \\( 1.2 < x \\leq 1.4 \\) (Data point 8)\r\n",
    "\r\n",
    "For simplicity, assume we do not split further for this example. Calculate \\( G_j \\) and \\( H_j \\) for each leaf:\r\n",
    "\r\n",
    "- Leaf 1:\r\n",
    "  - \\( G_1 = g_1 = 0 \\)\r\n",
    "  - \\( H_1 = h_1 = 2 \\)\r\n",
    "\r\n",
    "- Leaf 2:\r\n",
    "  - \\( G_2 = g_2 + g_3 = -2 + 0 = -2 \\)\r\n",
    "  - \\( H_2 = h_2 + h_3 = 2 + 2 = 4 \\)\r\n",
    "\r\n",
    "- Leaf 3:\r\n",
    "  - \\( G_3 = g_4 = -2 \\)\r\n",
    "  - \\( H_3 = h_4 = 2 \\)\r\n",
    "\r\n",
    "- Leaf 4:\r\n",
    "  - \\( G_4 = g_5 = 0 \\)\r\n",
    "  - \\( H_4 = h_5 = 2 \\)\r\n",
    "\r\n",
    "- Leaf 5:\r\n",
    "  - \\( G_5 = g_6 = -2 \\)\r\n",
    "  - \\( H_5 = h_6 = 2 \\)\r\n",
    "\r\n",
    "- Leaf 6:\r\n",
    "  - \\( G_6 = g_7 = 0 \\)\r\n",
    "  - \\( H_6 = h_7 = 2 \\)\r\n",
    "\r\n",
    "- Leaf 7:\r\n",
    "  - \\( G_7 = g_8 = -2 \\)\r\n",
    "  - \\( H_7 = h_8 = 2 \\)\r\n",
    "\r\n",
    "### Optimize Weights \\( w_j \\)\r\n",
    "\r\n",
    "The optimal weight \\( w_j \\) for each leaf \\( j \\) is given by:\r\n",
    "\r\n",
    "\\[\r\n",
    "w_j = -\\frac{G_j}{H_j + \\lambda}\r\n",
    "\\]\r\n",
    "\r\n",
    "Assume \\( \\lambda = 1 \\):\r\n",
    "\r\n",
    "- Leaf 1:\r\n",
    "  - \\( w_1 = -\\frac{0}{2 + 1} = 0 \\)\r\n",
    "\r\n",
    "- Leaf 2:\r\n",
    "  - \\( w_2 = -\\frac{-2}{4 + 1} = \\frac{2}{5} = 0.4 \\)\r\n",
    "\r\n",
    "- Leaf 3:\r\n",
    "  - \\( w_3 = -\\frac{-2}{2 + 1} = \\frac{2}{3} \\approx 0.67 \\)\r\n",
    "\r\n",
    "- Leaf 4:\r\n",
    "  - \\( w_4 = -\\frac{0}{2 + 1} = 0 \\)\r\n",
    "\r\n",
    "- Leaf 5:\r\n",
    "  - \\( w_5 = -\\frac{-2}{2 + 1} = \\frac{2}{3} \\approx 0.67 \\)\r\n",
    "\r\n",
    "- Leaf 6:\r\n",
    "  - \\( w_6 = -\\frac{0}{2 + 1} = 0 \\)\r\n",
    "\r\n",
    "- Leaf 7:\r\n",
    "  - \\( w_7 = -\\frac{-2}{2 + 1} = \\frac{2}{3} \\approx 0.67 \\)\r\n",
    "\r\n",
    "### Update Predictions\r\n",
    "\r\n",
    "Update the predictions \\(\\hat{y}_i\\) for each data point based on the leaf they fall into:\r\n",
    "\r\n",
    "- Data point 1 (Leaf 1): \\(\\hat{y}_1^{(t)} = \\hat{y}_1^{(0)} + w_1 = 0 + 0 = 0\\)\r\n",
    "- Data point 2 (Leaf 2): \\(\\hat{y}_2^{(t)} = \\hat{y}_2^{(0)} + w_2 = 0 + 0.4 = 0.4\\)\r\n",
    "- Data point 3 (Leaf 2): \\(\\hat{y}_3^{(t)} = \\hat{y}_3^{(0)} + w_2 = 0 + 0.4 = 0.4\\)\r\n",
    "- Data point 4 (Leaf 3): \\(\\hat{y}_4^{(t)} = \\hat{y}_4^{(0)} + w_3 \\approx 0 + 0.67 = 0.67\\)\r\n",
    "- Data point 5 (Leaf 4): \\(\\hat{y}_5^{(t)} = \\hat{y}_5^{(0)} + w_4 = 0 + 0 = 0\\)\r\n",
    "- Data point 6 (Leaf 5): \\(\\hat{y}_6^{(t)} = \\hat{y}_6^{(0)} + w_5 \\approx 0 + 0.67 = 0.67\\)\r\n",
    "- Data point 7 (Leaf 6): \\(\\hat{y}_7^{(t)} = \\hat{y}_7^{(0)} + w_6 = 0 + 0 = 0\\)\r\n",
    "- Data point 8 (Leaf 7): \\(\\hat{y}_8^{(t)} = \\hat{y}_8^{(0)} + w_7 \\approx 0 + 0.67 = 0.67\\)\r\n",
    "\r\n",
    "### Summary\r\n",
    "\r\n",
    "- \\("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
